{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6756871",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "1. Classify blog posts into their respective categories using a Naive Bayes text-classification model.\n",
    "2. Perform sentiment analysis (positive / neutral / negative) on the blog texts.\n",
    "3. Evaluate and interpret both the classification and the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f236f",
   "metadata": {},
   "source": [
    "# 2. Data Understanding\n",
    "\n",
    "We are given a CSV file `blogs.csv` with:\n",
    "\n",
    "- **Data** – the text content of each blog post\n",
    "- **Labels** – the category of the blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f25b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b386e9b",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d47fbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Exploration ---\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "\n",
      "                                                Data       Labels\n",
      "0  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
      "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism\n",
      "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  alt.atheism\n",
      "3  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
      "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  alt.atheism \n",
      "\n",
      "\n",
      "Dataset Information:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Data    2000 non-null   object\n",
      " 1   Labels  2000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.4+ KB\n",
      "None \n",
      "\n",
      "\n",
      "Distribution of Categories:\n",
      "Labels\n",
      "alt.atheism                 100\n",
      "comp.graphics               100\n",
      "talk.politics.misc          100\n",
      "talk.politics.mideast       100\n",
      "talk.politics.guns          100\n",
      "soc.religion.christian      100\n",
      "sci.space                   100\n",
      "sci.med                     100\n",
      "sci.electronics             100\n",
      "sci.crypt                   100\n",
      "rec.sport.hockey            100\n",
      "rec.sport.baseball          100\n",
      "rec.motorcycles             100\n",
      "rec.autos                   100\n",
      "misc.forsale                100\n",
      "comp.windows.x              100\n",
      "comp.sys.mac.hardware       100\n",
      "comp.sys.ibm.pc.hardware    100\n",
      "comp.os.ms-windows.misc     100\n",
      "talk.religion.misc          100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"blogs.csv\")\n",
    "\n",
    "print(\"--- Data Exploration ---\\n\")\n",
    "\n",
    "print(\"First 5 rows of the dataset:\\n\")\n",
    "\n",
    "print(df.head(),\"\\n\")\n",
    "\n",
    "print(\"\\nDataset Information:\\n\")\n",
    "print(df.info(),\"\\n\")\n",
    "\n",
    "print(\"\\nDistribution of Categories:\")\n",
    "print(df['Labels'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9138944",
   "metadata": {},
   "source": [
    "# 4. Custom Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370aa96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_stop_words = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
    "    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "    \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d76e868",
   "metadata": {},
   "source": [
    "# 5. Data Preprocessing\n",
    "Steps:\n",
    "- Lowercasing\n",
    "- Removing URLs, HTML tags, punctuation, digits\n",
    "- Tokenizing\n",
    "- Removing stop-words\n",
    "- Lemmatizing\n",
    "- Normalizing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e952d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/bunny/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/bunny/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in custom_stop_words and len(word) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Processed_Data'] = df['Data'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e228a",
   "metadata": {},
   "source": [
    "# 6. Feature Extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3aa998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Extraction ---\n",
      "Shape of TF-IDF matrix: (2000, 9450)\n",
      "Number of unique features: 9450\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1,2))\n",
    "X = tfidf_vectorizer.fit_transform(df['Processed_Data'])\n",
    "y = df['Labels']\n",
    "\n",
    "print(\"\\n--- Feature Extraction ---\")\n",
    "print(f\"Shape of TF-IDF matrix: {X.shape}\")\n",
    "print(f\"Number of unique features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af5af4",
   "metadata": {},
   "source": [
    "# 7. Train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a6c35fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Accuracy: 0.9210 ± 0.0064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "cv_scores = cross_val_score(nb_classifier, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"5-Fold Cross-Validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7e7f7",
   "metadata": {},
   "source": [
    "# 8. Sentiment Analysis (VADER + TextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa188a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/bunny/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentiment Analysis Results ---\n",
      "Combined_Sentiment\n",
      "Positive    1338\n",
      "Negative     641\n",
      "Neutral       21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment Distribution by Category:\n",
      "Combined_Sentiment        Negative  Neutral  Positive\n",
      "Labels                                               \n",
      "alt.atheism                     42        1        57\n",
      "comp.graphics                   16        0        84\n",
      "comp.os.ms-windows.misc         25        0        75\n",
      "comp.sys.ibm.pc.hardware        21        1        78\n",
      "comp.sys.mac.hardware           24        0        76\n",
      "comp.windows.x                  20        2        78\n",
      "misc.forsale                     9        5        86\n",
      "rec.autos                       27        1        72\n",
      "rec.motorcycles                 31        1        68\n",
      "rec.sport.baseball              27        2        71\n",
      "rec.sport.hockey                28        1        71\n",
      "sci.crypt                       29        0        71\n",
      "sci.electronics                 18        4        78\n",
      "sci.med                         39        0        61\n",
      "sci.space                       32        3        65\n",
      "soc.religion.christian          29        0        71\n",
      "talk.politics.guns              69        0        31\n",
      "talk.politics.mideast           69        0        31\n",
      "talk.politics.misc              50        0        50\n",
      "talk.religion.misc              36        0        64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def combined_sentiment(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'Neutral'\n",
    "    sia_score = sia.polarity_scores(text)['compound']\n",
    "    tb_score = TextBlob(text).sentiment.polarity\n",
    "    avg_score = (sia_score + tb_score)/2\n",
    "    if avg_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif avg_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Combined_Sentiment'] = df['Data'].apply(combined_sentiment)\n",
    "\n",
    "print(\"\\n--- Sentiment Analysis Results ---\")\n",
    "print(df['Combined_Sentiment'].value_counts())\n",
    "\n",
    "sentiment_distribution = df.groupby('Labels')['Combined_Sentiment'].value_counts().unstack(fill_value=0)\n",
    "print(\"\\nSentiment Distribution by Category:\")\n",
    "print(sentiment_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674eb86",
   "metadata": {},
   "source": [
    "# 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa74b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation ---\n",
      "Naive Bayes Classifier Performance Metrics:\n",
      "Accuracy: 0.9050\n",
      "\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.82      0.70      0.76        20\n",
      "           comp.graphics       0.94      0.85      0.89        20\n",
      " comp.os.ms-windows.misc       0.95      1.00      0.98        20\n",
      "comp.sys.ibm.pc.hardware       0.78      0.90      0.84        20\n",
      "   comp.sys.mac.hardware       1.00      0.90      0.95        20\n",
      "          comp.windows.x       0.90      0.95      0.93        20\n",
      "            misc.forsale       0.95      1.00      0.98        20\n",
      "               rec.autos       0.95      1.00      0.98        20\n",
      "         rec.motorcycles       0.95      0.95      0.95        20\n",
      "      rec.sport.baseball       1.00      1.00      1.00        20\n",
      "        rec.sport.hockey       1.00      1.00      1.00        20\n",
      "               sci.crypt       1.00      1.00      1.00        20\n",
      "         sci.electronics       0.89      0.85      0.87        20\n",
      "                 sci.med       0.95      0.90      0.92        20\n",
      "               sci.space       1.00      1.00      1.00        20\n",
      "  soc.religion.christian       0.83      1.00      0.91        20\n",
      "      talk.politics.guns       0.93      0.65      0.76        20\n",
      "   talk.politics.mideast       0.90      0.95      0.93        20\n",
      "      talk.politics.misc       0.93      0.70      0.80        20\n",
      "      talk.religion.misc       0.57      0.80      0.67        20\n",
      "\n",
      "                accuracy                           0.91       400\n",
      "               macro avg       0.91      0.91      0.91       400\n",
      "            weighted avg       0.91      0.91      0.91       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(\"Naive Bayes Classifier Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c1c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e9e235",
   "metadata": {},
   "source": [
    "# 10. Discussion & Reflection\n",
    "\n",
    "1. **Classifier Performance:** Naive Bayes with TF-IDF + bigrams performs robustly, achieving high precision, recall, and F1-scores. Some overlapping categories may have slightly lower scores.\n",
    "2. **Challenges:** Preprocessing text (removing noise, lemmatization) was crucial. Naive Bayes assumes feature independence, which is a limitation.\n",
    "3. **Sentiment Analysis:** Combining VADER and TextBlob provides more stable sentiment classification. Categories like sports show more positive sentiment; politics shows more neutral/negative sentiment.\n",
    "4. **Conclusion:** The workflow (preprocessing → TF-IDF → Naive Bayes → sentiment) is effective for classifying blogs and providing additional sentiment insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
